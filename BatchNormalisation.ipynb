{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BatchNormalisation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KmhQqswGPjhW","colab_type":"text"},"source":["(Just draft version, feel free to change)\n","Group Members: F. castanheira, C. Meo, M. Sala\n","\n","This is our blog plus code for the Reproducibility Project of the course CS4240 Deep Learning. The paper that we selected is: \"Batch Normalisation: Accelerating Deep Network Training by Reducing Internal Covariate Schift\" [1].\n","\n","This paper had a huge impact for the Deep Learning community, with over than 17000 citations. Training of Deep Neural Networks is often very hard, due to the vanishing/exploding gradient problem. Batch normalisation aims to correct it. The improvement in the results using batch normalisation is undoubatable. However, the paper received also some critics. For example, the term \"Covariate Shift\" introduce by the paper is definitively an illecit use of vocabulary and we found it quite confusing.\n","\n","\n","The focus was given to reproduce figure 1 of the paper [1].\n","For completness the picture together with its description is here reported.\n","\n","![alt text](https://drive.google.com/uc?id=11AP4tC-HW2diQA5dC1lDIO8CVh98CE35)\n","     \n","\n","Figure (a) represents the test accuracy on the MNIST dataset of a simple feedforward network both with and without bacth normalisation. On the x-axis there are the training steps (50K in total), and on the y-axis the accuracy. As it is possible to see, batch normalisation allow for a greater accuracy with lower training time. This is due to the distributions that the activations see. Indeed figure (b) and (c) depicts the input distribution to a typical sigmoid in the last hidden layer of the network. The 3 lines correspond to the 15, 50 and 85 [percentile](https://en.wikipedia.org/wiki/Percentile) of the inputs (for a definition of percentile see the wikipedia link). The consequence of batch normalisation is a smoother distribution which allow the network to perform better.\n","We decided to tackle this problem from two prospective: From one side a Pytorch version of the two networks of figure 1(a) was made, and on the other an implementation from Skratch using just Pytorch tensors for computational efficiency. Lastly, we tried to assess the robusteness of the results using a CNN on MNIST.\n","\n","\n","[1] S. Ioffe, C. Szegedy (2015). \"Batch Normalisation: Accelerating Deep Network Training by Reducing Internal Covariate Schift\""]},{"cell_type":"markdown","metadata":{"id":"hBta1OaSS08r","colab_type":"text"},"source":["Loading all the packages that are needed."]},{"cell_type":"code","metadata":{"id":"um7Jp7P8PSrn","colab_type":"code","outputId":"cb888f48-fd3e-4e75-dc4f-c349986936f4","executionInfo":{"status":"ok","timestamp":1587240334295,"user_tz":-120,"elapsed":3028,"user":{"displayName":"marco sala","photoUrl":"","userId":"07284782352463012974"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import os\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","# Here specify the path to your directory\n","!ls \"/content/gdrive/My Drive/DeepLearning\" \n","root_path = 'gdrive/My Drive/DeepLearning' \n","path ='/content/gdrive/My Drive/DeepLearning'\n","os.chdir(path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","BatchNormalisation.ipynb  figure1.PNG\t\t percentile_ffn_NO_BN.png\n","clip_figure1.PNG\t  FromScratch.ipynb\n","data\t\t\t  percentile_ffn_BN.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WOwyOwd6yBW8","colab_type":"code","outputId":"314930b0-05ac-4e8a-bb01-a059ca57de4e","executionInfo":{"status":"ok","timestamp":1587240334299,"user_tz":-120,"elapsed":3000,"user":{"displayName":"marco sala","photoUrl":"","userId":"07284782352463012974"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# test of drive\n","print(os.getcwd()) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/DeepLearning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HDO9pfimlXEx","colab_type":"code","outputId":"0df59896-1b19-4869-88ae-52c05faa2a30","executionInfo":{"status":"ok","timestamp":1587240334303,"user_tz":-120,"elapsed":2827,"user":{"displayName":"marco sala","photoUrl":"","userId":"07284782352463012974"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch \n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import time # to save the model with the date\n","\n","torch.manual_seed(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f3ea8ef0fb0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"JIs_I5uJS40l","colab_type":"text"},"source":["Loading the train and test loaders with bacthes of 60 as in the paper."]},{"cell_type":"code","metadata":{"id":"mLzZB9mFlgDf","colab_type":"code","colab":{}},"source":["transform = transforms.Compose( [transforms.ToTensor()] )                          \n","trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=60,     \n","                                          shuffle=True, num_workers=1)\n","testset = torchvision.datasets.MNIST(root='./data', train=False,\n","                                     download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=60,\n","                                         shuffle=False, num_workers=1)\n","\n","classes = ('0', '1', '2', '3',\n","           '4', '5', '6', '7', '8', '9')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhZn_21dTBY2","colab_type":"text"},"source":["The following two classes are inspired by section ... of the paper. "]},{"cell_type":"code","metadata":{"id":"LrILRIGOl1uO","colab_type":"code","outputId":"9799a4fb-5094-4985-c877-537ac0fd01b6","executionInfo":{"status":"error","timestamp":1587240954202,"user_tz":-120,"elapsed":871,"user":{"displayName":"marco sala","photoUrl":"","userId":"07284782352463012974"}},"colab":{"base_uri":"https://localhost:8080/","height":229}},"source":["# This is the net with Batch Normalisation  \n","class Net_BN(nn.Module):\n","    def __init__(self):\n","        super(Net_BN, self).__init__(lr=0.2, momentum=0.9)\n","        self.fc1 = nn.Linear(28 * 28, 100)             # first hidden layer\n","        self.bn1 = nn.BatchNorm1d(100) \n","        self.fc2 = nn.Linear(100, 100)                 # second hidden layer\n","        self.bn2 = nn.BatchNorm1d(100)\n","        self.fc3 = nn.Linear(100, 100)                 # third hidden layer\n","        self.bn3 = nn.BatchNorm1d(100)                \n","        self.fc4 = nn.Linear(100, 10)                  # output layer        \n","        self.lr = lr\n","        self.momentum = momentum\n","\n","        self.init_weights = self.fc1.weight.data      # to print a typical initialisation\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = optim.SGD(self.parameters(), lr=model.lr, momentum=model.momentum)\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","        self.name = 'ffn_BN'\n","\n","    def forward(self, x):\n","        x = x.to(self.device)\n","        x = torch.flatten(x, start_dim=1)\n","        x = torch.sigmoid(self.bn1(self.fc1(x)))\n","        x = torch.sigmoid(self.bn2(self.fc2(x)))\n","        x = self.bn3(self.fc3(x))     # dimension is (60, 100)\n","\n","        self.activation = x[:, 0]    # input to 10th sigmoid, tensor of size 60\n","        self.activations = x\n","        x = torch.sigmoid(x)\n","        \n","        x = self.fc4(x)\n","        return x\n","\n","# This is the net without Batch Normalisation \n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__(lr=0.2, momentum=0.9)\n","        self.fc1 = nn.Linear(28 * 28, 100)\n","        self.fc2 = nn.Linear(100, 100)\n","        self.fc3 = nn.Linear(100, 100)\n","        self.fc4 = nn.Linear(100, 10)\n","        \n","        self.lr = lr\n","        self.momentum = momentum\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = optim.SGD(self.parameters(), lr=model.lr, momentum=model.momentum)\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","        self.name = 'ffn_NO_BN'\n","\n","    def forward(self, x):\n","        x = x.to(self.device)\n","        x = torch.flatten(x, start_dim=1)\n","        x = torch.sigmoid(self.fc1(x))\n","        x = torch.sigmoid(self.fc2(x))\n","\n","        x = self.fc3(x)\n","\n","        self.activation = x[:, 0] \n","        self.activations = x\n","        x = torch.sigmoid(x)\n","\n","        x = self.fc4(x)\n","        return x  \n","\n","net_BN = Net_BN()\n","net    = Net()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1fb8ba69d560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNet_BN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet_BN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# first hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"JJBpGP2ctP44","colab_type":"text"},"source":["This has the porpuse of proving that the weights are already initialized to small values by default. Therefore we did not implement a different initialisation procedure."]},{"cell_type":"code","metadata":{"id":"M7yved9zsbP0","colab_type":"code","outputId":"87f6d119-f543-4744-bd98-fb86e988d8b7","executionInfo":{"status":"ok","timestamp":1587240337502,"user_tz":-120,"elapsed":5183,"user":{"displayName":"marco sala","photoUrl":"","userId":"07284782352463012974"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["init_weights = net_BN.init_weights\n","print(init_weights)\n","print(torch.max(init_weights))\n","print(torch.min(init_weights))\n","\n","# if we wan to initialize the weight differently\n","\n","## takes in a module and applies the specified weight initialization\n","def weights_init(m):\n","    '''Takes in a module and initializes all linear layers with weight\n","        values taken from a normal distribution.'''\n","\n","    classname = m.__class__.__name__\n","    # for every Linear layer in a model\n","    if classname.find('Linear') != -1:\n","        y = m.in_features\n","    # m.weight.data shoud be taken from a normal distribution\n","        m.weight.data.normal_(0.0,1/np.sqrt(y))\n","    # m.bias.data should be 0\n","        m.bias.data.fill_(0)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0184, -0.0158, -0.0069,  ...,  0.0068, -0.0041,  0.0025],\n","        [-0.0274, -0.0224, -0.0309,  ..., -0.0029,  0.0013, -0.0167],\n","        [ 0.0282, -0.0095, -0.0340,  ..., -0.0141,  0.0056, -0.0335],\n","        ...,\n","        [-0.0170, -0.0294, -0.0351,  ..., -0.0320, -0.0291, -0.0083],\n","        [ 0.0207, -0.0126,  0.0167,  ..., -0.0350, -0.0347, -0.0292],\n","        [ 0.0182,  0.0104,  0.0114,  ..., -0.0278, -0.0205,  0.0123]])\n","tensor(0.0357)\n","tensor(-0.0357)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xll1K4FaTQvH","colab_type":"text"},"source":["The function *train* is used to avoid repetition of code. It works for both the network with and without bacth normalisation. By default loss is printed every 1000 minibatches.\n","It is very important to use the command *model.eval()* when the purpose is to test the model (even if toch.no_grad() is used). The reason is that it tells to *batch_norm* to operate in testing mode."]},{"cell_type":"code","metadata":{"id":"ARHmsrUooJVN","colab_type":"code","colab":{}},"source":["def train(model, tainloader, testloader, epochs = 50, LOSS_EVERY=1000, TEST_EVERY=1000):\n","  '''This function train the model on the data provided by trainloader.\n","   Test on testloader every TEST_EVERY. \n","  '''\n","  print('...Training starts...')\n","\n","  #model.apply(weights_init)\n","  model.train()\n","  print(model)\n","  accuracies = []\n","  activation_list = []\n","  all_activations = []\n","  training_steps = 0\n","  for epoch in range(epochs):\n","    running_loss = 0.0\n","    for i, (inputs, labels) in enumerate(trainloader):\n","        training_steps += 1\n","        model.optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = model.criterion(outputs, labels.long().to(model.device))\n","        loss.backward()\n","        model.optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % LOSS_EVERY == LOSS_EVERY-1:    \n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10))\n","            running_loss = 0.0\n","\n","        #model.eval() \n","\n","        correct = 0\n","        if i % TEST_EVERY == TEST_EVERY-1:      \n","          with torch.no_grad():\n","              for (images, labels) in testloader:\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                correct += (predicted ==  labels.long().to(model.device)).sum().item()\n","              \n","              accuracies.append(100 * correct / len(testloader.dataset))   \n","\n","    activation_list.append(model.activation) # appending the activation here   \n","    all_activations.append(model.activations) \n","\n","  timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n","  model.name = f'net_accuracy_{accuracies[-1]}_lr={model.lr}_{timestr}' \n","\n","  torch.save(torch.save(the_model.state_dict() ) #, PATH)\n","  print('......Training is done......')\n","  return accuracies, activation_list, all_activations "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dD3fZc6mToi3","colab_type":"text"},"source":["Training the feedforward network without batch normalisation"]},{"cell_type":"code","metadata":{"id":"i7PgrmQK222G","colab_type":"code","outputId":"187047ae-4499-496c-cc36-cba478b4616d","executionInfo":{"status":"ok","timestamp":1587240728959,"user_tz":-120,"elapsed":396044,"user":{"displayName":"marco sala","photoUrl":"","userId":"07284782352463012974"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["accuracies, activation_list, all_activations = train(net, trainloader, testloader, epochs = 50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["........Training starts........\n","Net(\n","  (fc1): Linear(in_features=784, out_features=100, bias=True)\n","  (fc2): Linear(in_features=100, out_features=100, bias=True)\n","  (fc3): Linear(in_features=100, out_features=100, bias=True)\n","  (fc4): Linear(in_features=100, out_features=10, bias=True)\n","  (criterion): CrossEntropyLoss()\n",")\n","[1,  1000] loss: 231.235\n","[2,  1000] loss: 225.665\n","[3,  1000] loss: 135.182\n","[4,  1000] loss: 70.372\n","[5,  1000] loss: 47.197\n","[6,  1000] loss: 36.898\n","[7,  1000] loss: 29.893\n","[8,  1000] loss: 25.234\n","[9,  1000] loss: 21.866\n","[10,  1000] loss: 19.173\n","[11,  1000] loss: 16.918\n","[12,  1000] loss: 15.330\n","[13,  1000] loss: 13.875\n","[14,  1000] loss: 12.659\n","[15,  1000] loss: 11.558\n","[16,  1000] loss: 10.714\n","[17,  1000] loss: 9.913\n","[18,  1000] loss: 9.362\n","[19,  1000] loss: 8.577\n","[20,  1000] loss: 7.923\n","[21,  1000] loss: 7.527\n","[22,  1000] loss: 7.022\n","[23,  1000] loss: 6.572\n","[24,  1000] loss: 6.339\n","[25,  1000] loss: 5.816\n","[26,  1000] loss: 5.408\n","[27,  1000] loss: 5.160\n","[28,  1000] loss: 4.789\n","[29,  1000] loss: 4.518\n","[30,  1000] loss: 4.263\n","[31,  1000] loss: 4.015\n","[32,  1000] loss: 3.800\n","[33,  1000] loss: 3.542\n","[34,  1000] loss: 3.286\n","[35,  1000] loss: 3.150\n","[36,  1000] loss: 2.944\n","[37,  1000] loss: 2.725\n","[38,  1000] loss: 2.543\n","[39,  1000] loss: 2.424\n","[40,  1000] loss: 2.262\n","[41,  1000] loss: 2.037\n","[42,  1000] loss: 2.031\n","[43,  1000] loss: 1.846\n","[44,  1000] loss: 1.671\n","[45,  1000] loss: 1.586\n","[46,  1000] loss: 1.516\n","[47,  1000] loss: 1.416\n","[48,  1000] loss: 1.303\n","[49,  1000] loss: 1.212\n","[50,  1000] loss: 1.130\n","......Training is done......\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I_BV9cr2TtJd","colab_type":"text"},"source":["training the feedforward network with batch normalisation"]},{"cell_type":"code","metadata":{"id":"T3tlA978oOYX","colab_type":"code","outputId":"9f7cd38b-4f96-4648-86c4-48ab984643f9","colab":{"base_uri":"https://localhost:8080/","height":638}},"source":["accuracies_BN, activations_BN, all_activations_BN = train(net_BN, trainloader, testloader, epochs = 50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["........Training starts........\n","Net_BN(\n","  (fc1): Linear(in_features=784, out_features=100, bias=True)\n","  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc2): Linear(in_features=100, out_features=100, bias=True)\n","  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc3): Linear(in_features=100, out_features=100, bias=True)\n","  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc4): Linear(in_features=100, out_features=10, bias=True)\n","  (criterion): CrossEntropyLoss()\n",")\n","[1,  1000] loss: 46.216\n","[2,  1000] loss: 24.424\n","[3,  1000] loss: 18.589\n","[4,  1000] loss: 15.168\n","[5,  1000] loss: 12.691\n","[6,  1000] loss: 10.862\n","[7,  1000] loss: 9.546\n","[8,  1000] loss: 8.209\n","[9,  1000] loss: 7.178\n","[10,  1000] loss: 6.504\n","[11,  1000] loss: 5.912\n","[12,  1000] loss: 5.227\n","[13,  1000] loss: 4.839\n","[14,  1000] loss: 4.449\n","[15,  1000] loss: 4.178\n","[16,  1000] loss: 3.644\n","[17,  1000] loss: 3.392\n","[18,  1000] loss: 3.295\n","[19,  1000] loss: 2.806\n","[20,  1000] loss: 2.865\n","[21,  1000] loss: 2.341\n","[22,  1000] loss: 2.442\n","[23,  1000] loss: 2.285\n","[24,  1000] loss: 2.180\n","[25,  1000] loss: 1.851\n","[26,  1000] loss: 1.950\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gjHm6OcVT4ZF","colab_type":"text"},"source":["**plot_accuracy** and **plot_percentile** are functions that serve to reproduce the plots in figure 1 of [1]."]},{"cell_type":"code","metadata":{"id":"qAuwzYKE1jDU","colab_type":"code","colab":{}},"source":["def plot_accuracy(accuracies, accuracies_BN, title='', save=False, fname=''):\n","  plt.figure()\n","  plt.title(title)\n","  plt.plot(np.array(accuracies)/100, '--k', label='Without BN')\n","  plt.plot(np.array(accuracies_BN)/100, 'b',   label='With BN')\n","  plt.ylim([0.7, 1])\n","  plt.xlim([0, 50])\n","  plt.xticks([10, 20, 30, 40, 50], ('10K', '20K', '30K', '40K', '50K'))\n","  plt.legend()\n","  plt.grid()\n","\n","  if save:\n","    plt.savefig(fname, dpi=200)\n","\n","  plt.show()\n","\n","\n","def plot_percentile(activations, title='', debugging=False, save=False, fname=None):\n","    percentile_15 = []\n","    percentile_50 = []\n","    percentile_85 = []\n","\n","    for tensor in activations:\n","      percentile_15.append(np.percentile(tensor.cpu().detach().numpy(), 15))\n","      percentile_50.append(np.percentile(tensor.cpu().detach().numpy(), 50))\n","      percentile_85.append(np.percentile(tensor.cpu().detach().numpy(), 85))      \n","\n","    plt.figure()\n","    plt.title(title)\n","    plt.plot(percentile_15, label='15 percentile')\n","    plt.plot(percentile_50, label='50 percentile')\n","    plt.plot(percentile_85, label='85 percentile')\n","    plt.legend()\n","    plt.grid()\n","\n","    if save:\n","      plt.savefig(fname, dpi=200)\n","\n","    plt.show()\n","\n","    if debugging:\n","      return percentile_15, percentile_50, percentile_85\n","\n","\n","\n","def plot_mean_percentile(activations, title='', debugging=False, save=False, fname=None):\n","    percentile_15 = []\n","    percentile_50 = []\n","    percentile_85 = []\n","\n","    for tensors in activations:\n","      percentile_15.append(np.percentile(tensors.cpu().detach().numpy(), 15))\n","      percentile_50.append(np.percentile(tensors.cpu().detach().numpy(), 50))\n","      percentile_85.append(np.percentile(tensors.cpu().detach().numpy(), 85))      \n","\n","    plt.figure()\n","    plt.title(title)\n","    plt.plot(percentile_15, label=r'$15^{th}$ percentile')\n","    plt.plot(percentile_50, label=r'$50^{th}$ percentile')\n","    plt.plot(percentile_85, label=r'$85^{th}$ percentile')\n","    plt.legend()\n","    plt.grid()\n","\n","    if save:\n","      plt.savefig(fname, dpi=200)\n","\n","    plt.show()\n","\n","    if debugging:\n","      return percentile_15, percentile_50, percentile_85"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2HWZeQUz7jg","colab_type":"text"},"source":["This is taking a random neuron in the last hidden layer:"]},{"cell_type":"code","metadata":{"id":"hUJlu7fyVbJ-","colab_type":"code","colab":{}},"source":["plot_percentile(activation_list, title='NO BN')\n","plot_percentile(activations_BN,  title='WITH BN')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9k3f75A00A9b","colab_type":"text"},"source":["This is using np.percentile on all the last hidden layer:"]},{"cell_type":"code","metadata":{"id":"ztIOSNNj0owd","colab_type":"code","colab":{}},"source":["plot_mean_percentile(all_activations, title='NO BN', save=True, fname='percentile_ffn_NO_BN')\n","plot_mean_percentile(all_activations_BN, title='WITH BN', save=True, fname='percentile_ffn_BN')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NPy0v2t4_0n","colab_type":"code","colab":{}},"source":["plot_accuracy(accuracies, accuracies_BN, 'FFN', save=True, fname='fnn_accuracies')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdtWl_44q_2S","colab_type":"code","colab":{}},"source":["break\n","# It is suggested to implement CNN as well:\n","class Conv(nn.Module):\n","    def __init__(self):\n","        super(Conv, self).__init__(lr=0.02, momentum=0.99)\n","        self.conv1 = nn.Conv2d(1, 6, 3, padding=1) \n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = optim.SGD(self.parameters(), lr=lr, momentum=momentum)\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, x):\n","        x = torch.Tensor(x).to(self.device)\n","\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 7 * 7)\n","        x = F.relu(self.fc1(x))\n","  \n","        x = self.fc2(x)  \n","        self.activation = x  \n","        x = F.relu(x)\n","        \n","        x = self.fc3(x)\n","        return x\n","\n","class Conv_BN(nn.Module):\n","    def __init__(self):\n","        super(Conv_BN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n","        self.conv1_bn = nn.BatchNorm2d(6) \n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 3, padding=1)\n","        self.conv2_bn = nn.BatchNorm2d(16)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(16 * 7 * 7, 120)\n","        self.fc1_bn = nn.BatchNorm1d(120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc2_bn = nn.BatchNorm1d(84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.to(self.device)\n","\n","    def forward(self, x):\n","        x = torch.Tensor(x).to(self.device)\n","        \n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 7 * 7)\n","        x = F.relu(self.fc1_bn(self.fc1(x)))\n","\n","        x = self.fc2_bn(self.fc2(x))\n","        self.activations = x   \n","        x = F.relu(x)\n","\n","        x = self.fc3(x)\n","        return x\n","\n","conv_BN = Conv_BN()\n","conv = Conv()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynWB8UXQ8aAL","colab_type":"code","colab":{}},"source":["accuracies_cnn, activations_cnn = train(conv, trainloader, testloader, epochs = 30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIvTk7cn8d3H","colab_type":"code","colab":{}},"source":["accuracies_cnn_BN, activations_cnn_BN = train(conv_BN, trainloader, testloader, epochs = 30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyzML1L18maW","colab_type":"code","colab":{}},"source":["plot_accuracy(accuracies_cnn, accuracies_cnn_BN, title='CNN') # it seems convolutions outmatch batch normalisation for this \"simple\" CNN. Might be wrong tho.\n","plot_percentile(activations, 'Without BN')\n","plot_percentile(activations_BN, 'With BN')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0y-zzprYTM_","colab_type":"code","colab":{}},"source":["# If we want to save the accuracies to txt\n","\n","np.savetxt('accuracies.txt', accuracies)\n","np.savetxt('accuracies_BN.txt', accuracies_BN)\n","np.savetxt('accuracies_conv.txt', accuracies_cnn)\n","np.savetxt('accuracies_conv_BN.txt', accuracies_cnn_BN)\n"],"execution_count":0,"outputs":[]}]}